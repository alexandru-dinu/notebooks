{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Precision, Recall, F1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Precision\n",
    "$\n",
    "\\begin{aligned}\n",
    "\\mathrm{precision} = \\frac{TP}{TP + \\color{red}{FP}} = \\frac{(1\\to1)}{(1\\to \\color{red}{1}) + (0\\to \\color{red}{1})} = \\frac{TP}{\\text{predicted positive}}\n",
    "\\end{aligned}\n",
    "$\n",
    "\n",
    "- Fraction of correct predictions among **all predictions**.\n",
    "- How many predictions are indeed correct?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recall\n",
    "$\n",
    "\\begin{aligned}\n",
    "\\mathrm{recall} &= \\frac{TP}{TP + \\color{red}{FN}} = \\frac{(1\\to1)}{(\\color{red}{1}\\to 1) + (\\color{red}{1}\\to 0)} = \\frac{TP}{\\text{all positive}}\n",
    "\\end{aligned}\n",
    "$\n",
    "\n",
    "- Fraction of the correct predictions among **all examples**.\n",
    "- How many predictions are correct, from the total examples?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### High Precision & Low Recall\n",
    "- $\\equiv$ Low FP & <span style=\"color:red\"> High FN </span> **$\\to$ strict model**\n",
    "- low FP: low (pred = 1, actual = 0)\n",
    "- high FN: high (pred = 0, actual = 1)\n",
    "- the model finds fewer true instances, at the expense of not finding many such instances\n",
    "    - think of a search engine returning only a few (but correct!) of all the documents that are relevant to a query\n",
    "- the model returns few pos predictions, but those predictions are indeed correct"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "true = [0,0,0,1,1,1,1,1,1,1,1,1,1,1,1]\n",
    "pred = [0,0,0,0,0,0,0,0,0,0,1,0,0,1,1]\n",
    "precision = 3 / (3 + 0) = 1\n",
    "recall    = 3 / (3 + 9) = 0.25\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Low Precision & High Recall\n",
    "- $\\equiv$ <span style=\"color:red\"> High FP </span> & Low FN **$\\to$ permissive model**\n",
    "- high FP: high (pred = 1, actual = 0)\n",
    "- low FN: low (pred = 0, actual = 1)\n",
    "- the model will correctly find more true instances, at the expense of incorrectly flagging a larger fraction of examples (i.e. FP)\n",
    "- the model returns many positive predictions, but only a few are indeed correct"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "true = [0,0,0,0,0,0,0,0,0,1,1,1,1,1,1]\n",
    "pred = [1,1,1,1,1,1,1,1,1,1,1,1,1,1,1]\n",
    "precision = 6 / (6 + 9) = 0.4\n",
    "recall    = 6 / (6 + 0) = 1\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation\n",
    "\n",
    "```python\n",
    "def precision(true, pred, c, by_count=True):\n",
    "    if by_count:\n",
    "        return round(np.count_nonzero(pred[true == c] == c) / pred[pred == c].size, 2)\n",
    "    else:\n",
    "        return np.sum((true == c) * (pred == c)) / np.sum(pred == c)\n",
    "\n",
    "def recall(true, pred, c, by_count=True):\n",
    "    if by_count:\n",
    "        return round(np.count_nonzero(pred[true == c] == c) / true[true == c].size, 2)\n",
    "    else:\n",
    "        return np.sum((true == c) * (pred == c)) / np.sum(true == c)\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
